{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data Analysis & Preparation\n",
        "\n",
        "We begin by loading the raw datasets and merging them to create a unified view of \"Sales vs. Logistics Cost\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "try:\n",
        "    # The \"..\" means go up one level to the main folder\n",
        "    items = pd.read_csv('Data/olist_order_items_dataset.csv')\n",
        "    products = pd.read_csv('Data/olist_products_dataset.csv')\n",
        "    payments = pd.read_csv('Data/olist_order_payments_dataset.csv')\n",
        "    \n",
        "    if items.empty or products.empty or payments.empty:\n",
        "        raise ValueError(\"One or more datasets are empty. Check the content of the CSV files.\")\n",
        "    \n",
        "    print(\"Datasets loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV files not found. Check your 'data' folder location.\")\n",
        "except ValueError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Items Dataset: 112650 rows, 7 columns\n",
            "Missing values in Items dataset:\n",
            "order_id               0\n",
            "order_item_id          0\n",
            "product_id             0\n",
            "seller_id              0\n",
            "shipping_limit_date    0\n",
            "price                  0\n",
            "freight_value          0\n",
            "dtype: int64\n",
            "\n",
            "Products Dataset: 32951 rows, 9 columns\n",
            "Missing values in Products dataset:\n",
            "product_id                      0\n",
            "product_category_name         610\n",
            "product_name_lenght           610\n",
            "product_description_lenght    610\n",
            "product_photos_qty            610\n",
            "product_weight_g                2\n",
            "product_length_cm               2\n",
            "product_height_cm               2\n",
            "product_width_cm                2\n",
            "dtype: int64\n",
            "\n",
            "Payments Dataset: 103886 rows, 5 columns\n",
            "Missing values in Payments dataset:\n",
            "order_id                0\n",
            "payment_sequential      0\n",
            "payment_type            0\n",
            "payment_installments    0\n",
            "payment_value           0\n",
            "dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check shapes and missing values\n",
        "for name, df in {'Items': items, 'Products': products, 'Payments': payments}.items():\n",
        "    print(f\"{name} Dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "    print(f\"Missing values in {name} dataset:\\n{df.isna().sum()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean & Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered valid orders. Rows reduced from 112650 to 112647.\n",
            "Merged dataset created with 112647 rows and 15 columns.\n",
            "Cleaned dataset. Rows reduced from 112647 to 111035.\n"
          ]
        }
      ],
      "source": [
        "# --- MERGE & CLEAN ---\n",
        "## 1. Filter for valid paid orders\n",
        "if 'order_id' in payments.columns and 'order_id' in items.columns:\n",
        "    valid_orders = payments['order_id'].unique()\n",
        "    original_rows = items.shape[0]\n",
        "    items = items[items['order_id'].isin(valid_orders)]\n",
        "    print(f\"Filtered valid orders. Rows reduced from {original_rows} to {items.shape[0]}.\")\n",
        "else:\n",
        "    print(\"Error: 'order_id' column missing in one of the datasets.\")\n",
        "\n",
        "# 2. Merge Items with Products (Link Price to Weight)\n",
        "if 'product_id' in items.columns and 'product_id' in products.columns:\n",
        "    df = pd.merge(items, products, on='product_id', how='inner')\n",
        "    print(f\"Merged dataset created with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "else:\n",
        "    print(\"Error: 'product_id' column missing in one of the datasets.\")\n",
        "\n",
        "# 3. Drop Errors (Missing values or 0s)\n",
        "before_cleaning = df.shape[0]\n",
        "df = df.dropna(subset=['product_category_name', 'product_weight_g', 'price'])\n",
        "df = df[(df['product_weight_g'] > 0) & (df['price'] > 0)]\n",
        "print(f\"Cleaned dataset. Rows reduced from {before_cleaning} to {df.shape[0]}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Rows: 111035\n",
            "Cleaned Rows:  111032\n"
          ]
        }
      ],
      "source": [
        "# 4. Outlier Handling\n",
        "# We cap items at 30kg. Items heavier than this usually require special freight \n",
        "# logic, so we exclude them to focus on standard warehouse capacity.\n",
        "df_clean = df[df['product_weight_g'] <= 30000].copy()\n",
        "print(f\"Original Rows: {len(df)}\")\n",
        "print(f\"Cleaned Rows:  {len(df_clean)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aggregate & Save Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success! Created 'data/lp_parameters.csv' and 'data/capacity.txt'\n"
          ]
        }
      ],
      "source": [
        "# Aggregate\n",
        "lp_data = df_clean.groupby('product_category_name').agg({\n",
        "    'price': 'mean', # Pi: average revenue\n",
        "    'product_weight_g': 'mean', # wi : average  weight\n",
        "    'order_item_id': 'count' # Di : total historical Demand\n",
        "}).reset_index()\n",
        "# rename columns\n",
        "lp_data.columns = ['Category', 'P_i', 'W_i', 'D_i']\n",
        "lp_data.to_csv('Data/lp_parameters.csv', index=False)\n",
        "with open('Data/capacity.txt', 'w') as f:\n",
        "    f.write(str(df_clean['product_weight_g'].sum()))\n",
        "print(\"Success! Created 'data/lp_parameters.csv' and 'data/capacity.txt'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
